{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/khatiwada/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "import numpy as np\n",
    "import fasttext\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_model(model_name):\n",
    "    if model_name == \"llama3\":\n",
    "        # Load pre-trained LLaMA model and tokenizer\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "    elif model_name == \"mistral\":\n",
    "        # Load pre-trained Mistral model and tokenizer\n",
    "        model_loc = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_loc)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_loc)\n",
    "    elif model_name == \"bert\":\n",
    "        # Load pre-trained BERT model and tokenizer\n",
    "        model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    elif model_name == \"roberta\":\n",
    "        # Load pre-trained BERT model and tokenizer\n",
    "        model = AutoModel.from_pretrained(\"roberta-base\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "    elif model_name == \"fasttext\":\n",
    "        # Load pre-trained fastText model\n",
    "        model = fasttext.load_model(\"cc.en.300.bin\")\n",
    "        tokenizer = None  # fastText does not use a tokenizer\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model_name: {model_name}\")\n",
    "\n",
    "    # Add a padding token if it does not exist and if the model uses a tokenizer\n",
    "    if tokenizer is not None and tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "        model.resize_token_embeddings(len(tokenizer))  # Resize model embeddings to accommodate new pad token\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_each_cell_embeddings(texts, model_name, model, tokenizer):\n",
    "    if model_name in {\"llama3\", \"mistral\", \"bert\", \"roberta\"}:\n",
    "        # Tokenize the input texts with padding and truncation\n",
    "        inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "        # Get the last hidden state\n",
    "        with torch.no_grad():\n",
    "            if model_name == \"llama3\":\n",
    "                last_hidden_state = model.base_model(**inputs, output_hidden_states=True).last_hidden_state\n",
    "            elif model_name == \"mistral\":\n",
    "                last_hidden_state = model(**inputs, output_hidden_states=True).hidden_states[-1]\n",
    "            elif model_name in {\"bert\", \"roberta\"}:\n",
    "                last_hidden_state = model(**inputs, output_hidden_states=True).last_hidden_state\n",
    "        # Mask the padding tokens\n",
    "        attention_mask = inputs['attention_mask'].unsqueeze(-1)\n",
    "        masked_last_hidden_state = last_hidden_state * attention_mask\n",
    "\n",
    "        # Compute the average embedding for each sentence\n",
    "        sum_embeddings = masked_last_hidden_state.sum(dim=1)\n",
    "        count_non_pad_tokens = attention_mask.sum(dim=1)  # .unsqueeze(-1)\n",
    "        # Avoid division by zero (if a sequence only contains padding tokens)\n",
    "        count_non_pad_tokens = torch.clamp(count_non_pad_tokens, min=1)\n",
    "        average_embeddings = sum_embeddings / count_non_pad_tokens\n",
    "\n",
    "        # Convert to numpy for use with scikit-learn\n",
    "        average_embeddings_np = average_embeddings.detach().numpy()\n",
    "    \n",
    "    elif model_name == \"fasttext\":\n",
    "        average_embeddings_np = []\n",
    "        for text in texts:\n",
    "            tokens = text.split()  # Assuming the tokenizer is a simple space split\n",
    "            word_embeddings = [model.get_word_vector(token) for token in tokens]\n",
    "            if word_embeddings:\n",
    "                average_embedding = np.mean(word_embeddings, axis=0)\n",
    "            else:\n",
    "                average_embedding = np.zeros(model.get_dimension())  # Handle empty text case\n",
    "            average_embeddings_np.append(average_embedding)\n",
    "        average_embeddings_np = np.array(average_embeddings_np)\n",
    "\n",
    "    return average_embeddings_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_bipartite_matching(average_embeddings_1, average_embeddings_2, texts1, texts2, threshold=0.5, penalty=5.0):\n",
    "    \"\"\"\n",
    "    Apply bipartite matching with quality enhancement, allowing some texts to remain unmatched.\n",
    "\n",
    "    Parameters:\n",
    "    average_embeddings_1 (list or numpy array): Embeddings for the first set of texts.\n",
    "    average_embeddings_2 (list or numpy array): Embeddings for the second set of texts.\n",
    "    texts1 (list): List of texts corresponding to average_embeddings_1.\n",
    "    texts2 (list): List of texts corresponding to average_embeddings_2.\n",
    "    threshold (float): Cosine distance threshold for filtering matches.\n",
    "    penalty (float): High penalty cost for matching a text to a dummy.\n",
    "\n",
    "    Returns:\n",
    "    matching_results (list of tuples): Optimal matches as (text1, text2, distance) tuples.\n",
    "    combined_embeddings (list): Combined embeddings of the matched pairs and unmatched embeddings.\n",
    "    unmatched_texts1 (set): Set of unmatched texts from the first list.\n",
    "    unmatched_texts2 (set): Set of unmatched texts from the second list.\n",
    "    \"\"\"\n",
    "    num_texts1 = len(average_embeddings_1)\n",
    "    num_texts2 = len(average_embeddings_2)\n",
    "\n",
    "    # Compute cosine distance matrix\n",
    "    cosine_distance_matrix = cosine_distances(average_embeddings_1, average_embeddings_2)\n",
    "\n",
    "    # Augment the cosine distance matrix to allow for unmatched texts\n",
    "    augmented_size = num_texts1 + num_texts2\n",
    "    augmented_cosine_matrix = np.full((augmented_size, augmented_size), penalty)\n",
    "    augmented_cosine_matrix[:num_texts1, :num_texts2] = cosine_distance_matrix\n",
    "\n",
    "    # Apply Hungarian algorithm on the augmented matrix\n",
    "    row_indices, col_indices = linear_sum_assignment(augmented_cosine_matrix)\n",
    "\n",
    "    # Filter matches based on the threshold\n",
    "    matching_results = []\n",
    "    combined_embeddings = []\n",
    "    matched_texts1 = set()\n",
    "    matched_texts2 = set()\n",
    "\n",
    "    for row, col in zip(row_indices, col_indices):\n",
    "        if row < num_texts1 and col < num_texts2 and augmented_cosine_matrix[row, col] < threshold:\n",
    "            matching_results.append((texts1[row], texts2[col], augmented_cosine_matrix[row, col]))\n",
    "            combined_embedding = (average_embeddings_1[row] + average_embeddings_2[col]) / 2\n",
    "            combined_embeddings.append(combined_embedding)\n",
    "            matched_texts1.add(texts1[row])\n",
    "            matched_texts2.add(texts2[col])\n",
    "\n",
    "    # Add unmatched embeddings\n",
    "    unmatched_texts1 = set(texts1) - matched_texts1\n",
    "    unmatched_texts2 = set(texts2) - matched_texts2\n",
    "\n",
    "    unmatched_indices1 = [texts1.index(text) for text in unmatched_texts1]\n",
    "    unmatched_indices2 = [texts2.index(text) for text in unmatched_texts2]\n",
    "\n",
    "    for idx in unmatched_indices1:\n",
    "        combined_embeddings.append(average_embeddings_1[idx])\n",
    "\n",
    "    for idx in unmatched_indices2:\n",
    "        combined_embeddings.append(average_embeddings_2[idx])\n",
    "\n",
    "    return matching_results, combined_embeddings, unmatched_texts1, unmatched_texts2\n",
    "\n",
    "def apply_bipartite_matching_simple(average_embeddings_1, average_embeddings_2, texts1, texts2, threshold=0.5):\n",
    "    # Compute cosine distance matrix using scikit-learn\n",
    "    cosine_distance_matrix = cosine_distances(average_embeddings_1, average_embeddings_2)\n",
    "    # Apply Hungarian algorithm to find the optimal bipartite matching\n",
    "    row_indices, col_indices = linear_sum_assignment(cosine_distance_matrix)\n",
    "\n",
    "    # Filter matches based on the threshold\n",
    "    matching_results = []\n",
    "    combined_embeddings = []\n",
    "    for row, col in zip(row_indices, col_indices):\n",
    "        if cosine_distance_matrix[row, col] < threshold:\n",
    "            matching_results.append((texts1[row], texts2[col], cosine_distance_matrix[row, col]))\n",
    "            combined_embedding = (average_embeddings_1[row] + average_embeddings_2[col]) / 2\n",
    "            combined_embeddings.append(combined_embedding)\n",
    "    \n",
    "    # Add unmatched embeddings\n",
    "    unmatched_texts1 = set(texts1) - {pair[0] for pair in matching_results}\n",
    "    unmatched_texts2 = set(texts2) - {pair[1] for pair in matching_results}\n",
    "\n",
    "    unmatched_indices1 = [texts1.index(text) for text in unmatched_texts1]\n",
    "    unmatched_indices2 = [texts2.index(text) for text in unmatched_texts2]\n",
    "\n",
    "    for idx in unmatched_indices1:\n",
    "        combined_embeddings.append(average_embeddings_1[idx])\n",
    "\n",
    "    for idx in unmatched_indices2:\n",
    "        combined_embeddings.append(average_embeddings_2[idx])\n",
    "    return matching_results, combined_embeddings, unmatched_texts1, unmatched_texts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert\"\n",
    "model, tokenizer = load_embedding_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Bipartite Matching with Scores:\n",
      "Toronto -> Toronto with score: 0.0\n",
      "Berlinn -> Berlin with score: 0.12553995847702026\n",
      "Barcelona -> Barcelona with score: 2.384185791015625e-07\n",
      "\n",
      "Unmatched Texts from texts1:\n",
      "\n",
      "Unmatched Texts from texts2:\n",
      "Boston\n",
      "Optimal Bipartite Matching with Scores:\n",
      "Toronto -> Boston with score: 0.15037411451339722\n",
      "Berlinn -> Berlin with score: 0.1255398988723755\n",
      "Barcelona -> barcelna with score: 0.3933650255203247\n",
      "\n",
      "Unmatched Texts from texts1:\n",
      "\n",
      "Unmatched Texts from texts2:\n"
     ]
    }
   ],
   "source": [
    "texts1_list = [\"Berlinn\", \"Toronto\", \"Barcelona\"]\n",
    "texts2_list = [\"Toronto\", \"Boston\", \"Berlin\", \"Barcelona\"]\n",
    "texts3_list = [\"Berlin\", \"barcelna\", \"Boston\"]\n",
    "\n",
    "all_columns = [texts1_list, texts2_list, texts3_list]\n",
    "\n",
    "value_frequency = {}\n",
    "for column in all_columns:\n",
    "    for value in column:\n",
    "        if value in value_frequency:\n",
    "            value_frequency[value] += 1\n",
    "        else:\n",
    "            value_frequency[value] = 1\n",
    " \n",
    "first_column = all_columns.pop(0)\n",
    "for second_column in all_columns:\n",
    "    texts1 = list(set(first_column))\n",
    "    texts2 = list(set(second_column))\n",
    "    average_embeddings_1 = get_each_cell_embeddings(texts1, model_name, model, tokenizer)\n",
    "    average_embeddings_2 = get_each_cell_embeddings(texts2, model_name, model, tokenizer)\n",
    "\n",
    "    matching_results, combined_embeddings, unmatched_texts1, unmatched_texts2 = apply_bipartite_matching_simple(average_embeddings_1, average_embeddings_2, texts1, texts2, threshold = 1)\n",
    "    \n",
    "    # Print the matching results with their scores\n",
    "    print(\"Optimal Bipartite Matching with Scores:\")\n",
    "    for pair in matching_results:\n",
    "        print(f\"{pair[0]} -> {pair[1]} with score: {pair[2]}\")\n",
    "\n",
    "\n",
    "    # Print unmatched texts\n",
    "    print(\"\\nUnmatched Texts from texts1:\")\n",
    "    for text in unmatched_texts1:\n",
    "        print(text)\n",
    "\n",
    "    print(\"\\nUnmatched Texts from texts2:\")\n",
    "    for text in unmatched_texts2:\n",
    "        print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hnswlib\n",
    "# import numpy as np\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # Function to index embeddings using HNSW index\n",
    "# def index_embeddings_hnsw(embeddings, ef_construction = 200, M= 16, number_of_neighbors = 50):\n",
    "#     # Initialize HNSW index\n",
    "#     dim = embeddings.shape[1]  # Get the dimension of embeddings\n",
    "#     num_elements = embeddings.shape[0]  # Get the number of embeddings\n",
    "#     p = hnswlib.Index(space='cosine', dim=dim)\n",
    "#     p.init_index(max_elements=num_elements, ef_construction=ef_construction, M=M)\n",
    "#     number_of_neighbors = num_elements\n",
    "#     p.set_ef(number_of_neighbors)  # Set the number of neighbors to search during the query\n",
    "\n",
    "#     # Add embeddings to HNSW index\n",
    "#     p.add_items(embeddings)\n",
    "\n",
    "#     return p\n",
    "\n",
    "# # Function to find embeddings within threshold distance from a query embedding using HNSW index\n",
    "# def find_embeddings_within_threshold_hnsw(query_embedding, hnsw_index, k, embeddings, threshold):  # Pass embeddings as argument\n",
    "#     # Query HNSW index for candidate nearest neighbors\n",
    "#     labels, distances = hnsw_index.knn_query(query_embedding, k=k)    \n",
    "#     # Filter candidate indices based on cosine similarity threshold\n",
    "#     print(labels)\n",
    "#     print(distances)\n",
    "    \n",
    "#     # Filter candidate indices based on cosine distance threshold\n",
    "#     indices_within_threshold = []\n",
    "#     for i in range(0, len(labels)):\n",
    "#         for label, distance in zip(labels[i], distances[i]):\n",
    "#             if distance > threshold:\n",
    "#                 break\n",
    "#             indices_within_threshold.append((label, distance))\n",
    "#         # indices_within_threshold = [(label, distance) for label, distance in zip(labels[0], distances[0]) if distance <= threshold]\n",
    "\n",
    "#     return indices_within_threshold\n",
    "\n",
    "# # Example threshold value\n",
    "# threshold = 0.1\n",
    "\n",
    "# # Example query embedding (replace this with your actual query embedding)\n",
    "# # query_embedding = average_embeddings_np[2]\n",
    "# # Generate random embeddings of size (8, 4096) with dtype=np.float32\n",
    "# average_embeddings_np = np.random.rand(10000, 4096).astype(np.float32)\n",
    "# query_embedding =  average_embeddings_np # Generating a random query embedding of size (4096,) with dtype=np.float32\n",
    "\n",
    "# # Index embeddings using HNSW index\n",
    "# hnsw_index = index_embeddings_hnsw(average_embeddings_np)\n",
    "# k = hnsw_index.max_elements\n",
    "# k =20 \n",
    "# # Find embeddings within threshold distance from the query embedding using HNSW index\n",
    "# indices_within_threshold = find_embeddings_within_threshold_hnsw(query_embedding, hnsw_index,k, average_embeddings_np, threshold)  # Pass embeddings\n",
    "# # Print indices of embeddings within threshold distance\n",
    "# print(\"Indices of embeddings within threshold distance:\", indices_within_threshold)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
